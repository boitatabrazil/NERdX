{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Stack NLP from txt to ner app\n",
    "\n",
    "Neste script iremos transformar txt selecionado num arquivo em formato csv para a marcação de entidades nomeadas.\n",
    "\n",
    "Usaremos o pacote padrão do python para organizar txt em uma string\n",
    "\n",
    "Usaremos o pacote NLTK para a detecção de sentenças e para tokenizar\n",
    "\n",
    "Para o treinamento e uso do Postagger, usaremos o PerceptronTagger do NLTK para treinar o modelo usando o corpus Floresta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "nltk.download('floresta')\n",
    "from nltk.corpus import floresta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui iremos abrir o arquivo txt em uma variavel chamada data\n",
    "\n",
    "\n",
    "onde tem '../TXT/jardim.txt' você pode mudar o 'jardim' para o nome do txt que você criou no passo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../TXT/jardim.txt', 'r') as myfile:\n",
    "    data = myfile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ela já está em forma de string, porém cheia de ruído, você pode visualizar ela executando a célula abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está cheio de \\n e palavras quebradas! iremos corrigir isso usando regex (re) o pacote de expressões regulares do python\n",
    "\n",
    "lembrando que números de páginas e outros ruídos devem ser removidos manualmente!\n",
    "\n",
    "\n",
    "\n",
    "primeiro iremos usar a função sub para remover as linhas em branco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = re.sub('\\n\\n','\\n',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depois usaremos a mesma função para juntar as palavras que foram hifenizadas por quebra de texto.\n",
    "\n",
    "ex:\n",
    "\n",
    "pe-\n",
    "\n",
    "dro\n",
    "\n",
    "dentro da string ela está representada por 'pe-\\ndro', se removermos o '-\\n' teremos 'pedro' como resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = re.sub('-\\n','',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalmente podemos remover as quebras de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = re.sub('\\n','',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto! nossa string está limpa! você podem visualizar chamando a variavel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentece Detector\n",
    "\n",
    "Agora iremos separar nossa string em sentenças\n",
    "\n",
    "a célula abaixo carrega um modelo para o português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/portuguese.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos aplicar o modelo a nossa variavel data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenizer.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "você pode visualizar as sentenças chamando a variavel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "agora iremos separar cada sentença em tokens\n",
    "\n",
    "\n",
    "para isso criaremos um vetor vazio e salvaremos nele o output da função tokenize.word_tokenize para cada sentença\n",
    "\n",
    "a função tokenize.word_tokenize recebe um sentença e retorna um vetor de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for sent in sentences:\n",
    "    tokens.append(tokenize.word_tokenize(sent, language='portuguese'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "você pode visualizar os tokens chamando a variavel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postag\n",
    "\n",
    "Uma parte do codigo abaixo foi copiado do site: http://www.nltk.org/howto/portuguese_en.html referente a preparação do corpus floresta. Nele carregamos o corpus, organizamos os tokens e separamos em treino e teste, essa separação é fundamental, pois como vamos treinar nosso próprio modelo de Postag será avaliado os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_tag(t):\n",
    "    if \"+\" in t:\n",
    "        return t[t.index(\"+\")+1:]\n",
    "    else:\n",
    "        return t\n",
    "\n",
    "\n",
    "tsents = floresta.tagged_sents()\n",
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]\n",
    "train = tsents[100:]\n",
    "test = tsents[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos agora carregar a função que irá fazer o treinamento\n",
    "\n",
    "Ela é baseada em Perceptrons, um conceito fundamental de inteligência computacional que tenta simular a operação de um neurônio sendo a base das redes neurais modernas.\n",
    "\n",
    "Referências https://en.wikipedia.org/wiki/Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = PerceptronTagger(load=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos treinar o modelo (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos avaliar com a função evaluate. O score varia de 0 a 1, quanto mais proximo de 1 melhor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos usar o nosso modelo para fazer o PoStag em nossas sentenças\n",
    "\n",
    "iremos salvar em outro vetor chamado sentences_tagged pra não misturar as coisas rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tagged = []\n",
    "for i in tokens:\n",
    "    sentences_tagged.append(tagger.tag(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "você pode visualizar o PoStag chamando a variavel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tagged[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizar e Salvar\n",
    "\n",
    "agora iremos organizar tudo em uma matriz em que a primera coluna é o numero da sentença, a segunda o token, a terceira o PoStag e a quarta a NERtag. Como não taggeamos ainda colocaremos todas igual a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz = []\n",
    "for i in range(0,len(sentences_tagged)):\n",
    "    for j in range(0,len(sentences_tagged[i])):\n",
    "        matriz.append([i,sentences_tagged[i][j][0],sentences_tagged[i][j][1],0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "você pode visualizar a matriz chamando a variavel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos transformar essa matriz em um dataframe usando o pacote Pandas\n",
    "\n",
    "dataframe é uma estrutura melhor para visualizar e guardar diferentes tipos de dados\n",
    "\n",
    "inclusive podemos dizer o que cada coluna é dentro da estrutura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(matriz,columns=['Sentence','Token','PosTag','NERtag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "você pode visualizar o resultado usando a função head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos salvar esse dataframe em um csv!\n",
    "\n",
    "você pode mudar a parte 'jardimNER' para o nome do seu livro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../NER/jardimNER.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valeu!\n",
    "\n",
    "### made by Vinicius Sampaio/ Mardônio França from Boitatá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
